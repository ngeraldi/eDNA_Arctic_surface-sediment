---
title: "Arctic_surface_post_DADA2_filtering"
author: "Nathan R. Geraldi"
date: "Nov 17, 2020"
output: github_document
---

set table options
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## libraries
```{r libraries, message=FALSE, warning=FALSE}
#library(RColorBrewer)
#library(rdrop2)  # what is this used for?
library(vegan)
library(fields)
library(psych)
library(tidyverse) 
```

## functions
```{r functions, message=FALSE, warning=FALSE}
# function to remove rows with n number of NA's
delete.na <- function(DF, n=0) {
  DF[rowSums(is.na(DF)) <= n,]
}
```


## define universal variables
```{r define_universal}

stud_pat<-"Arctic_surface"  # matches study specific title from pipe (begining of files).
dir<-"/Users/nathangeraldi/Dropbox/"
out_file<-"Documents/KAUST/eDNA/R/pipe_summary"
# export  to project folder
export_file<-"Documents/KAUST/eDNA/R/csv/"
# name for csv that you will save at very end
export_name<-paste(stud_pat,"_filtered_data_all2.csv",sep="")
# summary_file for summary tables
summary_file<-"Documents/KAUST/eDNA/eDNA_manuscripts/Arctic surface"

## set some other universal variables

####  !!!! need to be in alphabetical order - tabs in sam_file should match these names too !!!!! also have "type"" column
## include one for each miseq - incease of 2 plates or replicates
primers<-c('18smini',"co1","euka02","euka03", "rbclmini")#   sort(primers)
n_primers<-length(primers)
## must be in alphabetical order
minboots<-c("insect",50,70,90)  # for getting all data minboots and insect data
min_insect_score<-0.80# set minimum score for insect taxa score
###################           filtering parameters
## filter based on occurance in samples
min_samples_include<-2  # used duing filtering SV's in < this number will be removed

## filter based on proportion in blanks
bl_prop_cut<-0.001  ## remove SV's with more than this propotion in max blanks (individual SV's) compared to sum(max blanks)
     # logic - more than 0.001 proportion of reads in worst blank situation than likely contamination..
      # example worst blank sum of 230,000  at 0.001 - remove SV > 230 in worst blank -- USED
      # example  worst blank sum of 230,000  at 0.01 - remove SV > 2,300 in worst blank

## filter based on proportion in samples
sam_sum_prop_cut<-0.0001 ## remove SV's with less than this propotion of reads per miseq
      # examples 10 million reads if 0.0001 then remove SV with lesst than 100 reads (remove ~25-50% SV) .......OK  USED
      # examples 10 million reads if 0.001 then remove SV with lesst than 1000 reads (remove ~60-80% SV) .......conservative
     # logic - if very low percent in sample samples, then likely errors.

## set minumum number for rarefaction -- some samples usually have very few reads, partucularly for cores
#    !!!!!!! estimate run script and look at rare_mins,..... then choose!!!  !!!!!!!! -- 1000 to 5000 ?????????
 min_num_rarefaction<-3000
#  stil run but NO longer used - now based on occurance within each miseq run - at later time.

#########################################
##   for summary tables
###  change from old to new names

primers_summary_old<-c('18smini',"co1","euka02","euka03", "rbclmini")  # note "__2" at end puts it before nothing
primer_names<-data.frame( "Primer1"=c("18S_V9","CO1","18S_V7","18S_V7_2", "RBCL"), "primer_pair"= primers_summary_old, stringsAsFactors = FALSE)
primers_summary<-c("18S_V9","CO1","18S_V7","18S_V7_2", "RBCL")

minboots_summary<-c(50,70,90) 
############
################### set file path and name of sample data
## each primer should have own sheet with name matching primers
## primer sheets first then location sheet (1 row per each sample location), then other sheets with relevent data
sam_file_path<-paste(dir,"Documents/KAUST/eDNA/Samples_Data/Arctic surface/Arctic_surface_data_all.xlsx", sep="") ## set sample data file

## number of columns in the primers specific sheets, should be the same for all primer pairs, 
num_sample_cols<-10  #### - add one to final, or will remove
##

```


## import data
Sample data excel sheet should include sheet for each primer pair - sample_id, sample_num, and miseq_sample_id column then other info
then a sheet with locations, then a sheet with other sample relevent data
It then imports data from DADA2 pipeline
```{r import}
# sample data -- will need Quality control !!! make sure sample_se make sense
sheets <- openxlsx::getSheetNames(sam_file_path)
sam_dat <- lapply(sheets,openxlsx::read.xlsx,xlsxFile=sam_file_path)  # mes1<-sam_dat[[2]]   names(mes1)  names(sam_dat) sort(sheets)
names(sam_dat) <- sheets   # add name to each list

##    move to next .Rmd - nothing to do here
#  locations<-sam_dat[[n_primers+1]]  # isolate locations  ! double check depending
sample_dat<-sam_dat$sample_data # isolate other data
#  dating<-sam_dat[[n_primers+3]]  # not used at this step
##
sam_dat <-sam_dat[1:n_primers]   # keep only primer sample data
sam_dat <- lapply(sam_dat, function(df) mutate_at(df, .vars="sample_ID", as.character))  # make sure sample_ID is character
sam_dat <- lapply(sam_dat, function(df) mutate_at(df, .vars="miseq_sample_id", as.character)) 
sample_sam<-bind_rows(sam_dat, .id = 'id')  # names(sample_sam)   ## bind al primer pair data into single datagrame

#  import taxonomy assigned to sequences  
# summary, seqtab and taxass    list.files()    unique(sam$type)
setwd(paste(dir,out_file,sep=""))
f_tax<-list.files(pattern = paste(stud_pat,".*taxass.*\\.csv",sep="")) ## 3 x number of primers
f_sum<-list.files(pattern = paste(stud_pat,".*summary\\.csv", sep=""))  # one per primer
f_rds<-list.files(pattern = paste(stud_pat,".*seqtab\\.rds", sep=""))
##   import all data into lists            names(sum)  names(tax_list)
###  may need to add  "sample1" to last column ????
tax_list<-setNames(lapply(f_tax, read.csv), tools::file_path_sans_ext(basename(f_tax)))  # mes1<-tax_list[[10]]  names(mes1)

## make sure tax_list names match primer names and sample_sam
intersect(primers, names(sam_dat))
# intersect(primers, names(tax_list))

# get index for insect taxonomy assigned and add taxonomy comparable to assigntaxonomy from DADA2 - make first letter upper case
tax_list_insect_num<-grep("insect", names(tax_list))
# fix taxonomy of insect    match insect name to dada2 naming
for (i in tax_list_insect_num){
     tax_list[[i]]<-tax_list[[i]] %>% 
       # mes1<-tax_list[[9]]   %>% 
       dplyr::rename(Superkingdom=superkingdom, Kingdom=kingdom,Phylum=phylum, Class=class, Order=order, Family=family, Genus=genus, Species=species ) %>% 
       mutate(X = taxID) %>%  ## add to match seq from tax assign
        dplyr::select(-representative,-taxID,-taxon,-rank) %>%
       mutate_if(is.factor, as.character) %>% 
       mutate_if(is.logical, as.character) %>%  ####. !!!!  needed if have column of all NA !!! 
      mutate_at(vars(Superkingdom:Species), list(~na_if(., ""))) %>% # convert blank to NA
     mutate_at(vars(Superkingdom:Species), funs(ifelse(!is.na(score) &  score < min_insect_score, NA, .))) %>%         ## min score turn taxa blank *** removed becuase error ***  -  min_insect_score !is.na(score) & 
      dplyr::select(-score, -contains("sample1")) ## 3 remove score   
     
}
 ## mes1<-tax_list[[17]]       names(mes1). 1,5 ,9, 13 , 17
       

# get summary of reads from filter steps of DADA2

summar<-setNames(lapply(f_sum, read.csv), tools::file_path_sans_ext(basename(f_sum)))  # names(summar) mes1<-summar[[1]] 
## get seqtab tables from dada2  -- sample_id are row names and colnames are SV sequence
sv_list<-setNames(lapply(f_rds, readRDS), tools::file_path_sans_ext(basename(f_rds))) # mes1<-sv_list[[5]]  row.names(mes1)
```

## tidy data

```{r tidy}
#######################################
# tidy     ! DADA2  summary tables !    table from dada2 pipe

## make column name lower for sample information
sample_dat<-sample_dat %>% 
    rename_all(tolower)


#######   tidy summary data     #################
## add column for primer and sample_id  mes<-summar[[1]]  mes<-mes$sample_id
for (i in seq_along(summar)){
  summar[[i]]["miseq_sample_id"] <- rownames(summar[[i]]) }  #  
#  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
## most are numbers so need to convert to sample_id from sample sheets, switching between sample_num and sample_id need to be checked !!!!!!!!!!!!1
## now called miseq_sample_id to clarify -- Nov 2020
#   !!!!!!!!!!!!!!

# bind into dataframe
sum2<-bind_rows(summar, .id = 'names(sum)')
names(sum2)[1]<-"study_primer"
##########
## get name of primer and clean up - !!!!!!!!!!    NEED TO CHECK each time  !!!!!! !!!! - must match tabs in sample excel sheet
###############
sum2 <-sum2 %>% 
  rename_all(tolower) %>% 
  mutate(primer=study_primer) %>%
  mutate(primer=gsub("^.*?_","",primer)) %>%  # remove everything before first "_"
  mutate(primer=gsub("_[^_]+$","",primer)) %>%   # remove after last "_"
   mutate(primer=gsub("^.*?_","",primer)) %>%  # remove everything before first "_" including _
  mutate(primer=gsub("^.*?_","",primer)) %>%  # remove everything before first "_" including id
  #mutate(primer=gsub("_","",primer)) %>%   # remove any "_"
  mutate(primer=gsub('(["_"])\\1+', '\\1', primer) ) %>% # removes one "_" if two occur in a row
  mutate(study_primer=gsub("_[^_]+$","",study_primer))  # remove after last "_", removes "summary"

##############################################################################
#########    tidy miseq specific data    then   combine with summary   ######### 
#   names(sam)  unique(sam$primer)  unique(sum2$primer)

# add total number of sequences per sample to each primer sheet
sample_sam<-sample_sam[,c(1:num_sample_cols)]# keep only common columns  names(sample_sam) was 11 for dammam
sam2<-sample_sam %>%
  rename_all(tolower) %>% 
  dplyr::rename(primer=id) %>% 
  mutate(sample_num=as.character(sample_num)) %>% 
  left_join(sum2[,7:9]) %>% # keep only nonchim,id,primer
  dplyr::rename(reads_per_sample_dada=nonchim)   ### reads after DADA2 filters

###### set up sample data-sam2 to merge with tax data
# set sample list for each primer  use sam2 by primer
sam_list<-split(sam2,f=sam2$primer) # names(sam_list)   # split data from each primer into list

#  get index to call column names of sample types   x<-sam_list[[1]]
#       !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
##      !!!!!!!!!    need to sort samples in alphabetical order    !!!!! see lines below
## for most miseqs we just use sample_num, during pipe this is ordered as character, so need to be aware
##  need for next step of indexing columns to specify sample type
sam_list<- lapply(sam_list, function(x) { 
  row_number_original<-c(1:length(as.numeric(row.names(x)))) 
  x$miseq_sample_id<-as.character(x$miseq_sample_id)
  x<-arrange(x,miseq_sample_id)
  row.names(x)<-NULL
  row_number<-c(1:length(as.numeric(row.names(x)))) # make index for choose columns of SVs - SV from DADA2 arranges columns by !!!alphabet !!!!! - need to arrange accordingly
  x<-cbind(x,row_number,row_number_original)    })
# replicate to match list with primers and nboots.
 sam_list<-rep(sam_list, each=length(minboots))
 #####
 
## make lists for each sample type, needed for filtering  x<-sample_index_list[[6]]  unique(sam2$type) x<-sam_list[[1]]
sample_index_list<- lapply(sam_list, function(x) { 
  x<-x$row_number[x$type=="sample"]     })
#
blank_index_list<- lapply(sam_list, function(x) { 
  x<-x$row_number[x$type=="extraction blank" | x$type=="pcr blank"]     })  # x<-blank_index_list[[3]] 
#
positive_index_list<- lapply(sam_list, function(x) { 
  x<-x$row_number[x$type=="positive control" | x$type=="mock"]     })
#
all_index_list<- lapply(sam_list, function(x) { 
  x<-x$row_number    })

### make SV list into LIST of dataframes ######################################
dat<-purrr::map(sv_list, data.frame)

```

## get cluster seqs (cluster)
```{r cluster}
 cluster_dat<- dat# make list

for (i in seq_along(dat)){
  cluster_dat[[i]]<- seqateurs::cluster_otus(dat[[i]], similarity = 0.97, cores = 4)
  
}

#. testing code
#.  names(dat)
##   head(dat[[1]])
## mes<-cluster_dat[[1]]
#  dum<-as.data.frame(dat[[1]])
#  mes<- seqateurs::cluster_otus(dum, similarity = 0.97, cores = 4)
#   mes3<-data.frame(mes$cluster, dum)
#  mes2<-unique(mes$cluster) # at 90% 1000, at 97% 5506 

```


## seq sample add (for cluster)
```{r seq sample}
# make rownames a column, rownames were sample identification, which we use sample number  -  mes<-dat[[1]]  mes$sample_num
# was above clustering but including cluster now it needs to be here - cluster funtion can't have sample id as column
dat<-purrr::map(dat, function(x) rownames_to_column(x, var="miseq_sample_id"))  #  mes<- rownames(dat[[3]]) names(dat) all_index_list[[3]]
```




## add sequ abund to taxonomy

```{r taxa_w_seq}
##     transpose seq table and merge to tax table   
##  dat has first col of miseq_sample_id then colnames is sequence - do not open, will take minutes to view
dat1<- lapply(dat, function(x) { 
        s<-names(x) # - sequences of SV
        id<-x[,1]  # sample_num/miseq_sample_id
        x<-t(x[,-1])  # transpose , remove sample_num      mes<-names(x)  mes[1]
        x<-unname(x)
        x<-data.frame(x)
        #colnames(x)<-paste("X",id, sep="") ;return(x)
        x$seq<-s[-1]  ;return(x)
        })                # mes<-dat1[[1]]   mes[,1]     names(dat1)    #   x<-dat[[3]]  x2<-dat1[[5]] names(x2) names(dat1[[3]])
# assign colnames - which are miseq_sample_id
for (i in seq_along(dat1)){
  colnames(dat1[[i]]) <- c(paste("X_",dat[[i]][,1],sep=""),"seq")
}
########  join sample/otu with taxon  ##############
##   mes5<-tax_list[[1]]  names(dat3)   names(tax_list)     mes<-dat2[[3]]  names(mes) length(rownames(mes))

## duplicate dat1 to match tax_list    names(dat1)
dat2<-rep(dat1, each=length(minboots))
minboot_seq<-rep(minboots, times=length(primers))
primers_seq<-rep(primers, each=length(minboots))
primers_minboot<-paste(primers_seq,minboot_seq, sep=";")

############  add with tax_list- for each 
####   changed to loop and merge - much safer and not worry about orders - merged on seq. and X
#dat3<-mapply(cbind,dat2,tax_list, SIMPLIFY=F)   # names(dat3) names(tax_list)  #   x<-dat2[[17]]  x2<-tax_list[[5]]
dat3<-dat2    #  i<-9
for (i in seq_along(dat2)){
  mes<-dat2[[i]]
  mes1<-tax_list[[i]]
  mes1$X<-as.character(mes1$X)
  mes2<-left_join(mes,mes1 , by = c("seq" = "X"))
  dat3[[i]]<-mes2
   }  #  

## add column for minboot
for (i in seq_along(dat3)){
    dat3[[i]]["primer"] <- primers_seq[i] 
    dat3[[i]]["minboot"] <- minboot_seq[i] # taxa assign id
    dat3[[i]]["seq.1"] <- c(1:length(row.names(dat3[[i]])))  ## sequence number id
    dat3[[i]]$primer_taxassign_id <- paste(dat3[[i]]$primer,dat3[[i]]$minboot,sep=";")## add new column for id primer&assign
}

##     x<-dat3[[1]] names(x)  mes<-x[-which(as.character(x$X) %in% x$seq),]  - mes should have 0 rows!
# x$X[1]  x$seq[1]
```

## add (cluster) numbers

```{r add cluster}

## duplicate cluster to match dat1 which was duped to match tax_list    names(dat1)
cluster_dat2<-rep(cluster_dat, each=length(minboots))

dat3.5<-dat3    #  i<-9  mess<-cluster_dat2[[2]]  messd<-dat3[[2]]
# colnames(messd) messd2<-messd[,90:109]
for (i in seq_along(dat3)){
  mes<-dat3[[i]]
  mes1<-cluster_dat2[[i]]
  mes2<-left_join(mes,mes1[,1:2] , by = c("seq" = "sequence"))
  dat3.5[[i]]<-mes2
   }  #  

dat3<-dat3.5
# messd<-dat3.5[[3]]  messd2<-messd[,90:110]
```



## begin filtering

```{r filter}
####    begin filter SV based on # samples and blanks
####################################################################################
      #  mes1<-dat3[[17]]  names(mes1)  mes2<-dat3[[2]]   # names(dat4)  mes0<-tax_list[[2]]  names(tax_list)
      #  hist(apply(mes1[,samples_index],1,function(x) length(which(x>0)))) # sum SV's per sample
      #  mes2<-mes1[apply(mes1[,samples_index],1,function(x) (length(which(x>0)))>=min_samples_include),]

#### remove SV's in < number of samples  - lapply through all lists
dat4<-dat3
after_n_occurance_per_sample <- vector("list", length(names(dat4)))# make dummy list
names(after_n_occurance_per_sample) <- primers_minboot
after_n_occurance_per_sample1 <- vector("list", length(names(dat4)))# make dummy list mes<-after_n_occurance_per_sample[[1]]
names(after_n_occurance_per_sample1) <- primers_minboot

for (i in seq_along(dat4)){   ##  i<-9   names(dat4[[17]])  names(dat4)
        mes1<-dat4[[i]]
        mes2<-mes1[apply(mes1[,sample_index_list[[i]]],1,function(x) (length(which(x>0))) >= min_samples_include),]
   
        after_n_occurance_per_sample[[i]]<-length(row.names(mes2))
        after_n_occurance_per_sample1[[i]]<-apply(mes2[,all_index_list[[i]]], 2, sum) # all_index_list was changed apr 23,2020
             dat4[[i]]<-mes2
      }     #  mes2<-dat4[[16]]  mes1<-dat4[[13]]  names(mes2)  names(mes1)   names(all_index_list)

#  find sample that is not common between index list and dat3 - common error   i<-18
#    setdiff(as.character(all_index_list[[i]] ), as.character(gsub("X_", "", names(dat3[[i]])[1:96] ) )  )

##############
#### remove SV's based on % in samples then by blanks - lapply through all lists 

 dat_filt <- dat4   # make copy for next step   names(dat4)  names(dat2[[1]])

# create blank lists for summary
 after_blank_filt <- vector("list", length(names(dat_filt)))# 
 names(after_blank_filt) <- primers_minboot
 after_min_sample_filt <- vector("list", length(names(dat_filt)))# m
 names(after_min_sample_filt) <- primers_minboot
  after_blank_filt1 <- vector("list", length(names(dat_filt)))# 
  names(after_blank_filt1) <- primers_minboot
 after_min_sample_filt1 <- vector("list", length(names(dat_filt)))# m
 names(after_min_sample_filt1) <- primers_minboot
  after_min_sample_filt_SVsam<-vector("list", length(names(dat_filt)))# m
      names(after_min_sample_filt_SVsam) <- primers_minboot
 after_blank_filt_SVsam<-vector("list", length(names(dat_filt)))# m
     names(after_blank_filt_SVsam) <- primers_minboot
 
 for (i in seq_along(dat_filt)){    #  i<-1
     mes1<-dat_filt[[i]] # mes1<-dat4[[10]]  names(mes1)   mes1<-dat_filt[[10]]   samples first ~90 columns
     
      sv_sum<- apply(mes1[,sample_index_list[[i]]], 1, sum) # get sum reads for each SV from samples    nix<-sv_sum[sv_sum>1000]
     mes1$sam_sum_prop<-sv_sum/sum(sv_sum)  # % for each sv of all reads   hist(log(sam_sum))
     sv_percent_cut_reads<- sum(sv_sum)*sam_sum_prop_cut   #  sum(sv_sum)*0.00001
     
     bl_max<- apply(mes1[,blank_index_list[[i]]], 1, max)  # get max reads for each SV from all blanks
     mes1$bl_max_prop<-bl_max/sum(bl_max)  #     sum(bl_max_prop)       hist(log(bl_max))  order(-bl_max_percent)
     #bl_mean<- apply(mes1[,blank_index], 1, mean)  # get max reads for each SV from all blanks
     #sam_max<- apply(mes1[,sample_index], 1, max) # get max reads for each SV from samples
     #sam_mean<- apply(mes1[,sample_index], 1, mean) # get max reads for each SV from samples
     #bl_per_max_sam<-bl_max/sam_max  # percent of max blank to max read of single samples
     #bl_per_sum<-bl_max/sam_sum   # percent of max blank to sum of smaples
     
     # use sam_sum_prop_cut ---- remove based on % reads in samples- consider 0.0001
     # logic - if very low percent in sample samples, then likely errors.
     mes2<-mes1[mes1$sam_sum_prop > sam_sum_prop_cut,] 
     
      #  use bl_prop_cut  ----- remove SV's with more than this proportion in max blanks compared to sum(max blanks)
     # logic - more than 0.001 percent of reads in worst blank situation than likely contamination..
     mes3<-mes2[mes2$bl_max_prop < bl_prop_cut,] 
     #
     mes4<-dplyr::select(mes3,-sam_sum_prop,-bl_max_prop)  # remove unnecessary columns
     dat_filt[[i]]<-mes4
     
    # get data for summary
     #  number of unique SV per miseq/taxaassign
     after_min_sample_filt[[i]]<-length(row.names(mes2))
     after_blank_filt[[i]]<-length(row.names(mes3))
     # number of unique SV per sample, why sum works, not sure, but it does
     after_min_sample_filt_SVsam[[i]]<-apply(mes2[,all_index_list[[i]]], 2, function(x) sum(x > 0))
     after_blank_filt_SVsam[[i]]<-apply(mes3[,all_index_list[[i]]], 2, function(x) sum(x > 0)) 
     ## number of reads per sample
      after_min_sample_filt1[[i]]<-apply(mes2[,all_index_list[[i]]], 2, sum) # 
     after_blank_filt1[[i]]<-apply(mes3[,all_index_list[[i]]], 2, sum) 
 }  
 # convert all factors to characters
 #  and
 # remove taxonomy that are not assigned to any taxonomy  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  after_taxa_clean <- vector("list", length(names(dat_filt)))# for calc of lost SVs
  names(after_taxa_clean) <- primers_minboot
   after_taxa_clean1 <- vector("list", length(names(dat_filt)))# for calc of lost reads
   names(after_taxa_clean1) <- primers_minboot
   after_taxa_clean1_SVsam <- vector("list", length(names(dat_filt)))# for calc of lost reads
   names(after_taxa_clean1_SVsam) <- primers_minboot
 #  mes1<-dat_filt[[12]]     names(mes1)   names(dat_filt)

    for (i in seq_along(dat_filt)){ 
    dat_filt[[i]]<-dat_filt[[i]] %>%   # i<-1
      tidyr::unite(lineage, Superkingdom:Species,sep=";",remove=F) %>%
      filter_at(vars(Family:Species), any_vars(complete.cases(.)))    %>%  # NA in all family,genus,species !!!!!!!!
      filter_at(vars(Phylum:Order), any_vars(complete.cases(.)))    %>%  # NA in all phylum through order  !!!!!!!!
      mutate_if(is.factor, as.character) %>% 
      dplyr::select(-lineage)
    
        # get data for summary
      after_taxa_clean[[i]]<-length(row.names(dat_filt[[i]]))# sum rows, count sum >0
      after_taxa_clean1[[i]]<-apply(dat_filt[[i]][,all_index_list[[i]]], 2, sum) 
      after_taxa_clean1_SVsam[[i]]<-apply(dat_filt[[i]][,all_index_list[[i]]], 2, function(x) sum(x > 0)) 
     }
# x<-dat_filt[[3]]     filter_at(vars(b, c), any_vars(complete.cases(.))   any_vars(!is.na(.))
#.  lapply(dat_filt, dim)
  
```


## sum reads by unique taxa
#########    !!!!!!!  no longer run.  !!!!!!!!!!!!
 and also gather - first time in long form, but still in list with each df being miseq
```{r sum_by_taxa, eval=FALSE, include=FALSE}
# sum all reads for each unique taxon and !   gather !!
 dat_filt_taxa<-dat_filt  # mes1<-dat_filt[[2]]  names(dat_filt)
### dummy lists
  after_taxa_sum <- vector("list", length(names(dat_filt_taxa)))# for calc of lost SVs
  names(after_taxa_sum) <- primers_minboot
    after_taxa_sum_SV_sam <- vector("list", length(names(dat_filt_taxa)))# for calc of lost SVs
  names(after_taxa_sum_SV_sam) <- primers_minboot

# sum reads by unique taxa within unique miseq runs  and gather
for (i in seq_along(dat_filt_taxa)){        # i<-12
  dat_filt_taxa[[i]]<-dat_filt_taxa[[i]] %>% 
   # mes1<-mes %>% 
    dplyr::select(-seq.1,-seq) %>% # make sure all columns are numeric after group_by
    unite(lineage, Superkingdom:Species, sep=";",remove=F) %>%
    dplyr::select(-Superkingdom,-Kingdom, -Phylum,-Class,-Order,-Family,-Class,-Genus,-Species,-primer,-minboot) %>%  # 
    group_by_at(vars(lineage,primer_taxassign_id)) %>% 
    summarise_all(list("sum")) %>%   # get sum for all same taxa
    gather(key="miseq_sample_id",value="reads", starts_with("X"))  %>% 
    mutate(miseq_sample_id=gsub("X_","",miseq_sample_id) )  # add sample id column
  # get richnes of taxa per miseq/taxaassign
    mes <- dat_filt_taxa[[i]] %>% 
      filter(reads>0) 
    after_taxa_sum[[i]]<- length(unique(mes$lineage))
    # get richnes per sample
    mes_s<-mes %>% 
      group_by(primer_taxassign_id, miseq_sample_id) %>% 
      summarize(SV=n())
    after_taxa_sum_SV_sam[[i]]<- mes_s[,2:3]
    
    }   # names(mes)  mes1<-dat_filt[[2]]   mes<-dat_filt_taxa[[2]]  names(dat_filt_taxa[[3]])  mes<-x$seq[x$seq.1 %in% c(1,8,2)]
          #  mes<- after_taxa_sum[[22]]  names(dat_filt_taxa)    mes<- after_taxa_sum_SV_sam[[11]]
```



## sum by (clusters)/taxa
 and also gather - first time in long form, but still in list with each df being miseq - changed line 521 and 523
 
```{r sum_by_taxa, echo=FALSE}
# sum all reads for each unique taxon and !   gather !!
 dat_filt_taxa<-dat_filt  # mes1<-dat_filt[[2]]  names(dat_filt)
### dummy lists
  after_taxa_sum <- vector("list", length(names(dat_filt_taxa)))# for calc of lost SVs
  names(after_taxa_sum) <- primers_minboot
    after_taxa_sum_SV_sam <- vector("list", length(names(dat_filt_taxa)))# for calc of lost SVs
  names(after_taxa_sum_SV_sam) <- primers_minboot

# sum reads by unique taxa within unique miseq runs  and gather
for (i in seq_along(dat_filt_taxa)){        # i<-12
  dat_filt_taxa[[i]]<-dat_filt_taxa[[i]] %>% 
   # mes1<-mes %>% 
    dplyr::select(-seq.1,-seq) %>% # make sure all columns are numeric after group_by
    ## unite(lineage, Superkingdom:Species, sep=";",remove=F) %>%
    ## change for cluster
    unite(lineage, c('Superkingdom','Kingdom','Phylum','Class','Order','Family','Class','Genus','Species', 'cluster'), sep=";",remove=F) %>%
    #.  see above, but nothing else
    dplyr::select(-Superkingdom,-Kingdom, -Phylum,-Class,-Order,-Family,-Genus,-Species, -cluster,-primer,-minboot) %>%  # 
    group_by_at(vars(lineage,primer_taxassign_id)) %>% 
    summarise_all(list("sum")) %>%   # get sum for all same taxa
    gather(key="miseq_sample_id",value="reads", starts_with("X"))  %>% 
    mutate(miseq_sample_id=gsub("X_","",miseq_sample_id) )  # add sample id column
  
  # get richnes of taxa per miseq/taxaassign
    mes <- dat_filt_taxa[[i]] %>% 
      filter(reads>0) 
    after_taxa_sum[[i]]<- length(unique(mes$lineage))
    # get richnes per sample
    mes_s<-mes %>% 
      group_by(primer_taxassign_id, miseq_sample_id) %>% 
      summarize(SV=n())
    after_taxa_sum_SV_sam[[i]]<- mes_s[,2:3]
    
    }   # names(mes)  mes1<-dat_filt[[2]]   mes<-dat_filt_taxa[[2]]  names(dat_filt_taxa[[3]])  mes<-x$seq[x$seq.1 %in% c(1,8,2)]
          #  mes<- after_taxa_sum[[22]]  names(dat_filt_taxa)    mes<- after_taxa_sum_SV_sam[[11]]
```



## tidy 2
bind data into one dataframe and add miseq sample data
```{r gather}
  ### simplify list into one dataframe and tidy    ##################################
 #. dat_filt_taxa_df<-do.call(rbind,dat_filt_taxa)
 dat_filt_taxa_df<-do.call(rbind,dat_filt_taxa)
 
#  names(dat_filt_taxa_df)  names(sam2)   names(mes5) unique(dat_filt_taxa_df$sample_id)

### merge with miseq sample data  - using sam2
 dat_filt_taxa_df<-dat_filt_taxa_df %>% 
  ungroup() %>% 
   separate(primer_taxassign_id, into=c("primer","minboot"), sep=";" , remove=FALSE) %>% 
   mutate(miseq_sample_id= as.character(miseq_sample_id)) %>%  # make character to join wit sam2, change back last line?????
  left_join(sam2) %>%  #, by=c("primer","sample_num")  unique(sam2$sample_id)
  mutate(sample_id_u=paste(primer,sample_num,minboot,sep=";")) %>%  ## switch unique sample id to num !!!!!!
  #mutate(primer_taxa_assign_id=paste(primer,minboot,sep="_")) %>% 
  mutate(lineage2=make.names(lineage)) %>% 
   mutate(lineage2=gsub('^X', '', lineage2)) %>%   # ^ replace everything but x with blank?
   mutate(lineage2=gsub('^\\.+|\\.+$', '', lineage2)) %>%  # remove leading or trailing "."
   group_by(sample_id_u) %>% 
   mutate(sample_sum_after_filt=sum(reads)) %>%  # get sum reads per sample after filter
   ungroup() 
 #  unique(dat_filt_taxa_df$primer)
#   mes<-dat_filt_taxa_df[dat_filt_taxa_df$primer_taxa_assign_id=="NA_NA",]  # unique(dat_filt_taxa_df$primer_taxa_assign_id)
  dat_filt_taxa_df$lineage2[1:10]
```




## rarify and diversity measures

```{r rar_and_div}
###  rarify  and simpson diversity ???    names(rar)    x<-rar[[12]]     x2<-rar2[[2]]

### get minumum number of reads per sample (only true samples).. for rarefy
rare_mins<-dat_filt_taxa_df %>%   # names(dat_filt_taxa_df)
  filter(type=="sample") %>% 
  dplyr::select(primer_taxassign_id, sample_id_u, reads) %>% 
  group_by(primer_taxassign_id,sample_id_u) %>% 
  summarise(sum_sample=sum(reads)) %>% 
  group_by(primer_taxassign_id) %>% 
  summarise(min_sample=min(sum_sample), low_quant=quantile(sum_sample, probs=0.25), median=median(sum_sample), mean=mean(sum_sample))

min_num_rarefaction<-as.vector(rare_mins$low_quant) 
sample_read_min<-as.vector(rare_mins$fiftypercent_lower) 

####      !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#   !!!!!!!!! take a look at rare_mins, most in cores have very low     !!!!!!!!!!!!!!!!!!!!
####      !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


## make list from data frame by primer and taxa assign id      names(dftd)
dftd<-dat_filt_taxa_df %>% 
    dplyr::select(primer_taxassign_id,sample_id_u, sample_id,lineage2,reads) %>% 
    split(list(.$primer_taxassign_id))


## rarify
rar2<-dftd
nam_rar2<-names(rar2)
j<-0  # i<-1
# fix taxonomy of insect    match insect name to dada2 nameing
for (i in nam_rar2){
     x<-rar2[[i]] #  i<-5
      j<-j+1 # for changing minimum sample size -min_num_rarefaction

  x1<-dplyr::select(x,sample_id_u,lineage2, reads)
  x1<-spread(x1, key=lineage2,value=reads) # spread so samples are rows
  ## get min number of reads per .. s
        #nn<-cbind(x1$sample_id_u,rowSums(x1[,-1])) 
        #nn<-data.frame(nn)
        #colnames(nn) <- c("sample_id", "r_sum")
        #nn$r_sum<-as.numeric(as.character(nn$r_sum)) #  hist(nn$r_sum[nn$r_sum<50000])   sum(nn$r_sum)*0.005
        #s_true<-min(nn$r_sum)  ## find min reads for rarefy
        #s<-ifelse(s_true<min_num_rarefaction,min_num_rarefaction,s_true)
  ###  rarefy
  x2<-data.frame(vegan::rrarefy(x1[,-1],sample=min_num_rarefaction[j]), check.names = FALSE)
  # fix incase nothing assigned  !!!!!!
  if (nrow(x2)<2) {
    x21<-data.frame(t(x2))
    x3<-bind_cols(x1[,1],x21)
  } else{ x3<-bind_cols(x1[,1],x2)}
  x4<-gather(x3, key="lineage2",value="rare_reads", -sample_id_u) # 
  rar2[[i]]<-x4
}

rar3<-bind_rows(rar2, .id = "primer_taxassign_id")  # combine rarified lists

## rarified diversity
rar_div<-lapply(rar2, function(x) { 
  #  x<-rar2[[12]]
  x1<-dplyr::select(x,sample_id_u,lineage2, rare_reads)
  x1<-spread(x1, key=lineage2,value=rare_reads) 
  s<-min(rowSums(x1[,-1]))
  x2<-data.frame(vegan::diversity(x1[,-1],index = "shannon"))
  #x3<-bind_cols(x1[,1],x2)
    # fix incase nothing assigned  !!!!!!
  if (nrow(x2)<2) {
    x3<-x1[,1]
    x3$NA.NA<-NA # if nothing IDed, then diversity is NA
  } else{ x3<-bind_cols(x1[,1],x2)}
  names(x3)[2]<-"shan_div_rare" # 
  x<-x3
})
rar_div2<-bind_rows(rar_div, .id = "primer_taxassign_id") # combine diveristy lists

## real diversity
div<-lapply(dftd, function(x) { 
  #  x<-rar2[[12]]
  x1<-dplyr::select(x,sample_id_u,lineage2, reads)
  x1<-spread(x1, key=lineage2,value=reads) 
  s<-min(rowSums(x1[,-1]))
  x2<-data.frame(vegan::diversity(x1[,-1],index = "shannon"))
  #x3<-bind_cols(x1[,1],x2)
    # fix incase nothing assigned  !!!!!!
  if (nrow(x2)<2) {
    x3<-x1[,1]
    x3$NA.NA<-NA # if nothing IDed, then diversity is NA
  } else{ x3<-bind_cols(x1[,1],x2)}
  names(x3)[2]<-"shan_div" # 
  x<-x3
})
div2<-bind_rows(div, .id = "primer_taxassign_id") # combine diveristy lists

##  combine rare data     names(dat6)
dat_filt_taxa_df<-dat_filt_taxa_df %>% 
  left_join(rar3) %>%  ## unique(dat5$primer_taxassign_id)  unique(rar3$primer_taxassign_id)
  left_join(rar_div2) %>%
  left_join(div2) %>%
  dplyr::select(-lineage2)

### richness and rarefied richness
rich<-dat_filt_taxa_df %>% 
  dplyr::select(primer_taxassign_id,sample_id_u,lineage,reads,rare_reads) %>% 
  group_by(primer_taxassign_id,sample_id_u) %>% 
  summarise(rich=length(reads[reads>0]),rich_rare=length(rare_reads[rare_reads>0]))

## join back in
dat_filt_taxa_df1<-dat_filt_taxa_df %>% 
  left_join(rich)

#dat5<-delete.na(dat4, 4)      unique(dat)
#   dat_filt_taxa_df2$lineage[1:10]
```



## add sample info.
```{r sample_dat}
## this is the first use of sample_data from all data excel
## this add sample info with big df
#   names(dat_filt_taxa_df)    names(sample_dat)
#   use sample_ID in case plate layout (sample_num) changed among primers
# remove sample_num from sample_dat to ensure good join
sample_dat1 <- sample_dat  #%>% 
#    dplyr::select(-sample_id_old1)  ### likely remove for other studies
  
  # add in sample information
 dat_filt_taxa_df2 <- dat_filt_taxa_df1 %>% 
    left_join(sample_dat1)
  
  
```



## summary_tables
does not use dat_filt_taxa_df1
NEED to re-run all script if already ran or will get lots of errors
-could fix by giving new names ?????????
```{r summary_table}

# tidy summary of read changes from dada2 pipe
  #  names(summar)       mes1<-summar[[1]]

## use sample_sam to join correct numbers and sample_id  -- set above, don't rerun
# dat_in_id<- c("Dammam__18s_stoeck_summary","Dammam__co1_summary")

dada_summary<-summar %>% 
    purrr::map(data.frame) %>% 
    bind_rows(. , .id = "primer_taxassign_id") 


dada_summary2 <- dada_summary %>%   # names(dada_summary2)
  dplyr::select(-tabled) %>% 
  # warning OK -- ignore
  #  need to check depending on study label
  #separate(primer_taxassign_id, into=c("study","primer_pair1","primer_pair2", "remove")  , sep="_", remove=FALSE) %>%   # 
  separate(primer_taxassign_id, into=c("study","remove1","primer_pair1","primer_pair2", "remove")  , sep="_", remove=FALSE) %>%   # 
  
  mutate(primer_pair= if_else(primer_pair2=="18s","18s_stoeck", primer_pair2)) %>% 
  dplyr::select(-remove, -remove1, -study, -primer_pair1, -primer_pair2, -primer_taxassign_id) %>% 
  rename(primer_removed=input , denoised = denoised, FandR_merged=merged, bimeras_removed=nonchim)
  

##  for reads
#   mes1<-mes[[1]]
##     get summery per sample    ####
# 
nnn<-"after_n_occurance_per_sample"   #   mes1<-after_n_occurance_per_sample1[[1]]  mes1<-mes[[1]]
mes<-after_n_occurance_per_sample1
mes<-purrr::map(mes, data.frame)  # make into list of dataframes
mes<-purrr::map(mes, function(x) rownames_to_column(x, var="sample_num")) # make rownames a column
mes<-bind_rows(mes, .id = "primer_taxassign_id") # make into single data frame with name of list = id.
names(mes)[3]<-nnn  ## assign nnn to column name
after_n_occurance_per_sample1 <- mes
# 
nnn<-"after_min_sample_filt"
mes<-after_min_sample_filt1
mes<-purrr::map(mes, data.frame)  # make into dataframe
mes<-purrr::map(mes, function(x) rownames_to_column(x, var="sample_num")) # make rownames a column
mes<-bind_rows(mes, .id = "primer_taxassign_id") # names(mes)
names(mes)[3]<-nnn
after_min_sample_filt1 <- mes
# 
nnn<-"SV_after_min_sample_filt"
mes<-after_min_sample_filt_SVsam
mes<-purrr::map(mes, data.frame)  # make into dataframe
mes<-purrr::map(mes, function(x) rownames_to_column(x, var="sample_num")) # make rownames a column
mes<-bind_rows(mes, .id = "primer_taxassign_id") # names(mes)
names(mes)[3]<-nnn
after_min_sample_filt_SVsam <- mes
# 
nnn<-"after_blank_filt"
mes<-after_blank_filt1
mes<-purrr::map(mes, data.frame)  # make into dataframe
mes<-purrr::map(mes, function(x) rownames_to_column(x, var="sample_num")) # make rownames a column
mes<-bind_rows(mes, .id = "primer_taxassign_id") # names(mes)
names(mes)[3]<-nnn
after_blank_filt1 <- mes
# 
nnn<-"SV_after_blank_filt"
mes<-after_blank_filt_SVsam
mes<-purrr::map(mes, data.frame)  # make into dataframe
mes<-purrr::map(mes, function(x) rownames_to_column(x, var="sample_num")) # make rownames a column
mes<-bind_rows(mes, .id = "primer_taxassign_id") # names(mes)
names(mes)[3]<-nnn
after_blank_filt_SVsam <- mes
# 
nnn<-"after_taxa_clean"
mes<-after_taxa_clean1
mes<-purrr::map(mes, data.frame)  # make into dataframe
mes<-purrr::map(mes, function(x) rownames_to_column(x, var="sample_num")) # make rownames a column
mes<-bind_rows(mes, .id = "primer_taxassign_id") # names(mes)
names(mes)[3]<-nnn
after_taxa_clean1 <- mes
# 
nnn<-"SV_after_taxa_clean"
mes<-after_taxa_clean1_SVsam
mes<-purrr::map(mes, data.frame)  # make into dataframe
mes<-purrr::map(mes, function(x) rownames_to_column(x, var="sample_num")) # make rownames a column
mes<-bind_rows(mes, .id = "primer_taxassign_id") # names(mes)
names(mes)[3]<-nnn
after_taxa_clean1_SVsam <- mes
# 
nnn<-"SV_after_taxa_sum"
mes<-after_taxa_sum_SV_sam
mes<-purrr::map(mes, data.frame)  # make into dataframe
#mes<-purrr::map(mes, function(x) rownames_to_column(x, var="sample_num")) # make rownames a column
mes<-bind_rows(mes, .id = "primer_taxassign_id") # names(mes)
names(mes)[3]<-nnn
after_taxa_sum_SV_sam <- mes
# changed july 2021 to fix join below
after_taxa_sum_SV_sam$miseq_sample_id <- as.numeric(after_taxa_sum_SV_sam$miseq_sample_id)
names(after_taxa_sum_SV_sam)[2] <- "sample_num"


## combine all tables
filter_summary_per_sample<-after_n_occurance_per_sample1 %>%    # names(filter_summary_per_sample)
  left_join(after_n_occurance_per_sample1) %>% 
  left_join(after_min_sample_filt1) %>% 
  left_join(after_blank_filt1) %>% 
  left_join(after_taxa_clean1) %>% 
  #
  left_join(after_min_sample_filt_SVsam) %>% 
  left_join(after_blank_filt_SVsam) %>% 
  left_join(after_taxa_clean1_SVsam) %>%
  #
  mutate(sample_num= as.numeric( gsub('X_', '', as.character(sample_num))) ) %>%  # remove "X_" from sample_num
  #
  left_join(after_taxa_sum_SV_sam) %>%
  #
  separate(primer_taxassign_id, into=c("primer_pair", "taxa_assign")  , sep=";", remove=FALSE) %>%   # split primer_taxassign_id to match with dada2_summary     names(filter_summary_per_sample)
  mutate(sample_num=as.character(sample_num)) %>% # need for next join
  left_join(dada_summary2, by=c("primer_pair" = "primer_pair", "sample_num" = "miseq_sample_id")) %>% 
  mutate(sample_num=as.numeric(sample_num))  %>% # change back for consistancy
  dplyr::select(primer_taxassign_id,primer_pair,taxa_assign,sample_num, primer_removed, filtered, 
         denoised, FandR_merged, bimeras_removed, 
         after_n_occurance_per_sample, after_min_sample_filt, after_blank_filt , after_taxa_clean,
         SV_after_min_sample_filt, SV_after_blank_filt, SV_after_taxa_clean, SV_after_taxa_sum)   # reorder columns


###########################
## clculate unique SV's after filters per miseq/taxaassign
## these have 1 number per miseq Removed Feb. 24, 2020!!!, if need go to Dammam scripts ?????

## TO DO ----- remove the following from above..
mes<-after_n_occurance_per_sample    # mes2<-mes[[1]] names(mes)
mes<-after_blank_filt
mes<-after_min_sample_filt
mes<-after_taxa_clean
mes<-after_taxa_sum


###########################################
#### combine all filtering into per primer/taxa assign     ####    unique(filter_summary_per_primer_minboot$sample_id_simple)
filter_summary_per_primer_minboot <- filter_summary_per_sample %>%  #   names(filter_summary_per_primer_minboot)
    left_join(sample_sam[,1:5], by = c("primer_pair" = "id", "sample_num" = "sample_num") ) %>%   # join with smaple_sam to get consistant sample names
    filter( !(type=="mock" | type =="positive control")) %>%  ## remove moc  unique(filter_summary_per_primer_minboot$type)
    mutate(type2=if_else(type =="extraction blank" | type=="pcr blank", "blanks", "samples", type)) %>% #unique(filter_summary_per_primer_minboot$type)
    group_by(primer_taxassign_id, primer_pair, taxa_assign, type2)  %>% 
    #summarize_at(vars(primer_removed:after_taxa_clean), sum) %>% 
    summarise_each(funs(mean=mean(., na.rm = TRUE), se=plotrix::std.error), primer_removed:SV_after_taxa_sum)  %>% 
  ## add data on unique SV per miseq  ??  - no  !! removed
    #left_join(after_n_occurance_per_sample) %>% 
    #left_join(after_min_sample_filt)  %>% 
    #left_join(after_blank_filt) %>%  
    #left_join(after_taxa_clean) %>% 
    #left_join(after_taxa_sum)     %>% 
    #left_join(primer_names)     %>%  # add good primer names
    #select(-primer_pair) %>%   # names(filter_summary_per_primer_minboot)
    rename(eDNA = type2, primers = primer_pair) %>% 
    filter(taxa_assign %in% minboots_summary) %>%  ###    !!!!!   remove insect !!!!!!
    # filter(primers %in% primers_summary) %>%  ###    !!!!!   keep used primers !!!!!!
    #mutate(primers=factor(primers, levels= primers_summary)) %>% 
    arrange(primers, desc(taxa_assign), desc(eDNA) )
  

#################### makes summary tables to then be exported
# names(filter_summary_per_primer_minboot)

DADA<-filter_summary_per_primer_minboot %>% 
    ungroup() %>% 
    distinct(primers,eDNA, .keep_all = TRUE) %>% 
    dplyr::select(primers,eDNA,primer_removed_mean, primer_removed_se,filtered_mean,filtered_se,denoised_mean,denoised_se,
           FandR_merged_mean,FandR_merged_se,bimeras_removed_mean,bimeras_removed_se) %>% 
   arrange(primers, desc(eDNA) )
  

# if filtered by minimum number of samples then --   after_min_sample_filt_mean  and    unique_SV_after_min_sample_filt
post_DADA_sum<-filter_summary_per_primer_minboot %>% 
    ungroup() %>% 
    dplyr::select(primers,taxa_assign, eDNA, after_n_occurance_per_sample_mean, after_n_occurance_per_sample_se,
           after_blank_filt_mean, after_blank_filt_se, after_taxa_clean_mean, after_taxa_clean_se,
           SV_after_min_sample_filt_mean,SV_after_min_sample_filt_se, 
           SV_after_blank_filt_mean,  SV_after_blank_filt_se,
           SV_after_taxa_clean_mean, SV_after_taxa_clean_se,
           SV_after_taxa_sum_mean, SV_after_taxa_sum_se ) %>%  
   arrange(primers, desc(taxa_assign), desc(eDNA))




```
example  https://stackoverflow.com/questions/29821841/dplyr-summarise-each-standard-error-function
summarise_each(funs(mean, se=plotrix::std.error), hp:drat) 


## check different controls
```{r control}

mes_blank <- filter_summary_per_sample %>%  #   names(filter_summary_per_primer_minboot)
    left_join(sample_sam[,1:5], by = c("primer_pair" = "id", "sample_num" = "sample_num") ) %>%   # join with smaple_sam to get consistant sample names
    filter( !(type=="mock" | type =="positive control")) %>%  ## remove moc  unique(filter_summary_per_primer_minboot$type)
    mutate(type2=if_else(type =="extraction blank" | type=="pcr blank", "blanks", "samples", type)) %>% #unique(filter_summary_per_primer_minboot$type)
    group_by(primer_taxassign_id, primer_pair, taxa_assign, type)  %>% 
    #summarize_at(vars(primer_removed:after_taxa_clean), sum) %>% 
    summarise_each(funs(mean=mean(., na.rm = TRUE), se=plotrix::std.error), primer_removed:SV_after_taxa_sum)  %>% 
  ## add data on unique SV per miseq  ??  - no  !! removed
    #left_join(after_n_occurance_per_sample) %>% 
    #left_join(after_min_sample_filt)  %>% 
    #left_join(after_blank_filt) %>%  
    #left_join(after_taxa_clean) %>% 
    #left_join(after_taxa_sum)     %>% 
    #left_join(primer_names)     %>%  # add good primer names
    #select(-primer_pair) %>%   # names(filter_summary_per_primer_minboot)
    rename(eDNA = type, primers = primer_pair) %>% 
    filter(taxa_assign %in% minboots_summary) %>%  ###    !!!!!   remove insect !!!!!!
    # filter(primers %in% primers_summary) %>%  ###    !!!!!   keep used primers !!!!!!
    #mutate(primers=factor(primers, levels= primers_summary)) %>% 
    arrange(primers, desc(taxa_assign), desc(eDNA) ) %>% 


   ungroup() %>% 
    distinct(primers,eDNA, .keep_all = TRUE) %>% 
    dplyr::select(primers,eDNA,primer_removed_mean, primer_removed_se,filtered_mean,filtered_se,denoised_mean,denoised_se,
           FandR_merged_mean,FandR_merged_se,bimeras_removed_mean,bimeras_removed_se) %>% 
   arrange(primers, desc(eDNA) )




```






## export unfilt data
```{r export_1}

# unfilter data
#saveRDS(dat3, Dammam_data_not_filtered)
data.table::fwrite(dat_filt_taxa_df2,paste(dir,export_file,export_name,sep=""),row.names=F, sep=",")

# summary tables
sjPlot::tab_df(DADA, alternateRowColors=FALSE, describe=FALSE,digits = 0, file=paste(dir,summary_file,"/DADA2_read_summary.html",sep="") )
 
sjPlot::tab_df(post_DADA_sum, alternateRowColors=FALSE, describe=FALSE,digits = 0, file=paste(dir,summary_file,"/Post_DADA2_read_summary.html",sep="")  )
 
```






